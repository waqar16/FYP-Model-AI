{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Lambda, Flatten, Dense\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import PIL\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "3/3 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Initialize the MTCNN detector\n",
    "detector = MTCNN()\n",
    "\n",
    "# Path to your image folder\n",
    "image_folder = 'Database_of_Images'\n",
    "\n",
    "# List to store detected faces\n",
    "faces = []\n",
    "\n",
    "# Loop through images and detect faces\n",
    "for filename in os.listdir(image_folder):\n",
    "    img = cv2.imread(os.path.join(image_folder, filename))\n",
    "    \n",
    "    # Detect faces using the MTCNN detector\n",
    "    result = detector.detect_faces(img)\n",
    "    \n",
    "    if result:\n",
    "        for face_info in result:\n",
    "            x, y, width, height = face_info['box']\n",
    "            face = img[y:y+height, x:x+width]\n",
    "            faces.append(face)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the pre-trained InceptionResnetV1 model\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Define the transformation to apply to your face images\n",
    "transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "encodings = []\n",
    "for image in faces:\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = np.around(image, decimals=12)\n",
    "    encoding = resnet(image)\n",
    "    encoding /= torch.norm(encoding, p=2)\n",
    "    encodings.append(encoding)\n",
    "# database = {}\n",
    "# database[\"Salman Khan\"] = img_to_encoding(\"Database_of_Images/OIP.jpg\", FRmodel)\n",
    "# database[\"Kareena Kapoor\"] = img_to_encoding(\"Database_of_Images/R (1).jpg\", FRmodel)\n",
    "# database[\"Katrina Kaif\"] = img_to_encoding(\"Database_of_Images/R (2).jpg\", FRmodel)\n",
    "# database[\"Shahrukh Khan\"] = img_to_encoding(\"Database_of_Images/R.jpg\", FRmodel)\n",
    "# database[\"Shaheen Afridi\"] = img_to_encoding(\"Database_of_Images/Shaheen-Afridi-2.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "new_img = cv2.imread('karena.jpg')\n",
    "new_result = detector.detect_faces(new_img)\n",
    "\n",
    "\n",
    "# Check if a face was detected\n",
    "if new_result:\n",
    "    # Get the coordinates of the first detected face\n",
    "    x, y, width, height = new_result[0]['box']\n",
    "\n",
    "    # Extract the face region from the image\n",
    "    detected_face = new_img[y:y+height, x:x+width]\n",
    "\n",
    "    # Now, 'detected_face' contains the pixel values of the detected face\n",
    "else:\n",
    "    print(\"No face detected in the image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the detected face using OpenCV\n",
    "cv2.imshow('Detected Face', detected_face)\n",
    "cv2.waitKey(0)  # Wait until a key is pressed\n",
    "cv2.destroyAllWindows()  # Close the OpenCV window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_new = transform(detected_face)\n",
    "image_new = image_new.unsqueeze(0)\n",
    "image_new = np.around(image_new, decimals=12)\n",
    "new_encoding = resnet(image_new)\n",
    "new_encoding /= torch.norm(new_encoding, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3418177\n",
      "1.1324469\n",
      "Match found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have already defined 'target_encoding' and 'encodings' correctly.\n",
    "\n",
    "# Set a threshold for matching\n",
    "threshold = 1.2  # You can adjust this based on your needs\n",
    "\n",
    "# Initialize variables for minimum distance and identity\n",
    "min_dist = float('inf')\n",
    "\n",
    "# Initialize match as False\n",
    "match = False\n",
    "\n",
    "# Calculate the distance between the target encoding and each reference encoding\n",
    "for ref_encoding in encodings:\n",
    "    dist = torch.norm(new_encoding - ref_encoding).detach().numpy()\n",
    "    print(dist)\n",
    "    # Check if the current distance is less than the threshold\n",
    "    if dist <= threshold:\n",
    "        match = True  # Match found\n",
    "        break\n",
    "\n",
    "if match:\n",
    "    print(f\"Match found.\")\n",
    "else:\n",
    "    print(\"No match found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
